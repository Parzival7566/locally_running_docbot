{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) based Language Model (LLM) application for analyzing 10-K financial documents. The app uses local LLM integration, document processing, vector store creation, and a Streamlit interface to provide an interactive question-answering system about company financial information.\n",
    "Key components of this project include:\n",
    "\n",
    "Document processing of 10-K PDF files\n",
    "Vector store creation for efficient information retrieval\n",
    "Integration of a local LLM for question answering\n",
    "Development of a conversational chain for context-aware responses\n",
    "Streamlit web application for user interaction\n",
    "\n",
    "Let's explore each component in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Constants\n",
    "In this section, we import all necessary libraries and define the constants used throughout the application. These include paths for data storage, model configurations, and other parameters that control the behavior of our app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries and Set Constants\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "from PyPDF2 import PdfReader\n",
    "import asyncio\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import math \n",
    "import re\n",
    "\n",
    "# Constants\n",
    "PDF_DIRECTORY = \"data/\"\n",
    "VECTOR_STORE_FILENAME = \"faiss_index\"\n",
    "BATCH_SIZE = 100\n",
    "MODEL_PATH = \"openhermes-2.5-mistral-7b.Q6_K.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embeddings and LLM\n",
    "Here, we set up two crucial components of our RAG system:\n",
    "\n",
    "*Embeddings*: We use HuggingFace's sentence transformers to create embeddings for our document chunks. These embeddings allow us to perform semantic similarity searches.\n",
    "*Local LLM*: We initialize a local LLM using the LlamaCpp library. This model will be responsible for generating human-like responses based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Embeddings and LLM\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=4096,\n",
    "    n_batch=512,\n",
    "    n_gpu_layers=-1,\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    "    verbose=True,\n",
    "    use_mlock=True,\n",
    "    use_mmap=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing\n",
    "Document processing is a critical step in our RAG pipeline. It involves two main functions:\n",
    "\n",
    "```get_pdf_text()```: This function extracts text from PDF files.\n",
    "\n",
    "```get_text_chunks()```: This function splits the extracted text into manageable chunks for processing.\n",
    "\n",
    "These functions prepare our 10-K documents for embedding and storage in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Document Processing Functions\n",
    "\n",
    "def get_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Ingestion\n",
    "Vector store ingestion is the process of creating and storing vector representations (embeddings) of our document chunks. This section includes:\n",
    "\n",
    "```embed_batch()```: A function to embed batches of text chunks.\n",
    "```create_and_save_vector_store()```: A function that processes all PDFs, creates embeddings, and stores them in a FAISS index.\n",
    "```load_vector_store()```: A function to load an existing vector store.\n",
    "\n",
    "This process allows for efficient similarity search when answering user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Vector Store Ingestion\n",
    "\n",
    "async def embed_batch(batch_chunks):\n",
    "    return embeddings.embed_documents(batch_chunks)\n",
    "\n",
    "async def create_and_save_vector_store(pdf_dir):\n",
    "    global vector_store\n",
    "    all_chunks = []\n",
    "\n",
    "    # Get all chunks from PDF files\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            text = get_pdf_text(pdf_path)\n",
    "            chunks = get_text_chunks(text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "    # Batch Embedding\n",
    "    num_batches = math.ceil(len(all_chunks) / BATCH_SIZE)\n",
    "    embedding_tasks = []\n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * BATCH_SIZE\n",
    "        end_idx = min((batch_num + 1) * BATCH_SIZE, len(all_chunks))\n",
    "        batch_chunks = all_chunks[start_idx:end_idx]\n",
    "        embedding_tasks.append(embed_batch(batch_chunks))\n",
    "\n",
    "    # Gather Embedding Results\n",
    "    embeddings_list = await asyncio.gather(*embedding_tasks)\n",
    "    embeddings_list = [embedding for batch in embeddings_list for embedding in batch]\n",
    "\n",
    "    # Create FAISS Index\n",
    "    text_embeddings = list(zip(all_chunks, embeddings_list))\n",
    "    vector_store = FAISS.from_embeddings(text_embeddings, embeddings)\n",
    "    vector_store.save_local(VECTOR_STORE_FILENAME)\n",
    "\n",
    "def load_vector_store():\n",
    "    global vector_store\n",
    "    vector_store = FAISS.load_local(VECTOR_STORE_FILENAME, embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine Development\n",
    "The query engine is responsible for understanding user questions and generating appropriate responses. The key component here is the get_conversational_chain() function, which:\n",
    "\n",
    "1. Defines a prompt template to guide the LLM's responses.\n",
    "2. Creates a question-answering chain that combines the LLM with our retrieval system.\n",
    "\n",
    "This setup allows for context-aware responses that draw information from the relevant parts of our 10-K documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Query Engine Development\n",
    "\n",
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    You are a helpful AI assistant designed to answer questions about financial documents of different companies. \n",
    "    You have been provided with information from Form 10-K filings of multiple companies.\n",
    "    \n",
    "    Use the provided context to answer the question as accurately as possible. \n",
    "    Whenever the user asks a question about google, answer the question with the context of the alphabet document as google is a subsidary of alphabet; provide a disclaimer for this as well.\n",
    "    If the question asks for a comparison, make sure to highlight the differences and similarities between the companies.\n",
    "    If you cannot answer the question from the given context, say \"I'm sorry, I don't have enough information to answer that.\"\n",
    "    \n",
    "    Always start your answer with the company name(s) relevant to the question.\n",
    "    If asked about specific financial figures, provide the exact numbers from the context if available.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit App Structure\n",
    "While we can't run the Streamlit app directly in this notebook, this section outlines the structure of our web application. Key components include:\n",
    "\n",
    "1. Setting up the Streamlit page and initializing session state.\n",
    "2. Handling PDF processing and vector store creation/loading.\n",
    "3. Implementing the chat interface for user interaction.\n",
    "4. Processing user queries and displaying responses.\n",
    "\n",
    "The Streamlit app provides an intuitive interface for users to interact with our RAG-based LLM system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Streamlit App\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"10-K Analyzer\", page_icon=\"üìà\")\n",
    "    st.title(\"üîç Analyze Financial Documents\")\n",
    "\n",
    "    # Session State Initialization\n",
    "    if 'buffer_memory' not in st.session_state:\n",
    "        st.session_state.buffer_memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "    if \"messages\" not in st.session_state.keys():\n",
    "        st.session_state.messages = [\n",
    "            {\"role\": \"assistant\", \"content\": \"Hi! I can help analyze Form 10-K documents. Ask me anything! üòä\"}\n",
    "        ]\n",
    "\n",
    "    # PDF Processing & Vector Store\n",
    "    if not os.path.exists(VECTOR_STORE_FILENAME):\n",
    "        with st.spinner(\"Processing PDFs...\"):\n",
    "            progress_bar = st.progress(0, text=\"Starting...\")\n",
    "            asyncio.run(create_and_save_vector_store(PDF_DIRECTORY)) \n",
    "            progress_bar.progress(1.0, text=\"PDFs processed and vector store created!\")\n",
    "            st.success(\"PDFs processed and vector store created!\")\n",
    "\n",
    "    if os.path.exists(VECTOR_STORE_FILENAME) and vector_store is None:\n",
    "        with st.spinner(\"Loading vector store...\"):\n",
    "            load_vector_store()\n",
    "\n",
    "    # Chat Interaction\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.write(message[\"content\"])\n",
    "\n",
    "    if prompt := st.chat_input(\"Enter your question about the 10-K filings...\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(prompt)\n",
    "\n",
    "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                try:\n",
    "                    docs = vector_store.similarity_search(prompt, k=5)\n",
    "                    chain = get_conversational_chain()\n",
    "                    response = chain.invoke({\"input_documents\": docs, \"question\": prompt}, return_only_outputs=True)\n",
    "                    st.write(response[\"output_text\"])\n",
    "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"output_text\"]})\n",
    "                except Exception as e:\n",
    "                    st.error(f\"An error occurred while processing your request: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Application\n",
    "To run the Streamlit application:\n",
    "\n",
    "1. Ensure all required libraries are installed (pip install -r requirements.txt).\n",
    "2. Place your 10-K PDF files in the data/ directory.\n",
    "3. Run the command streamlit run app.py in your terminal.\n",
    "4. Open the provided URL in your web browser to interact with the app.\n",
    "\n",
    "*Note*: The first run may take some time as it processes the PDFs and creates the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Improvements\n",
    "This notebook demonstrates a functional RAG-based LLM system for analyzing 10-K financial documents. Some potential areas for future improvement include:\n",
    "\n",
    "1. Implementing more advanced text chunking strategies for better context retrieval.\n",
    "2. Exploring different embedding models to improve semantic search accuracy.\n",
    "3. Fine-tuning the local LLM on financial domain data for more accurate responses.\n",
    "4. Adding features like document comparison or time series analysis of financial metrics.\n",
    "5. Implementing user authentication and document upload functionality in the Streamlit app.\n",
    "\n",
    "By continually refining and expanding this system, we can create an increasingly powerful tool for financial document analysis and information retrieval."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
